Different initializations lead to very different results
Random initialization is used to break symmetry and make sure different hidden units can learn different things
Resist initializing to values that are too large!
He initialization works well for networks with ReLU activations
